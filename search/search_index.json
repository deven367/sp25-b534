{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"deepep/","title":"DeepEP","text":"<p>DeepEP: Communication Library for Mixture-of-Experts (MoE)</p> <p>Class Project Discussion Layout</p>"},{"location":"deepep/#1-introduction-to-mixture-of-experts-moe","title":"1. Introduction to Mixture-of-Experts (MoE)","text":"<ul> <li>What is MoE?</li> <li>A neural network architecture where multiple \"expert\" subnetworks process different subsets of input tokens.</li> <li>A gating network dynamically routes inputs to the most relevant experts.</li> <li> <p>Enables scaling model capacity without proportional compute cost.</p> </li> <li> <p>Key Challenges in Distributed MoE Training</p> </li> <li>Communication Overhead: Routing tokens between experts across GPUs/nodes.</li> <li>Load Imbalance: Hotspots if some experts receive more tokens than others.</li> <li>Synchronization: Efficiently aggregating gradients/results from distributed experts.</li> </ul>"},{"location":"deepep/#2-overview-of-deepep","title":"2. Overview of DeepEP","text":"<ul> <li>Purpose: A high-performance communication library optimized for MoE workloads.</li> <li>Developed by: DeepSeek to address bottlenecks in distributed MoE training.</li> <li>Key Goals:</li> <li>Minimize latency in cross-GPU communication.</li> <li>Balance computational load across experts.</li> <li>Maximize throughput for large-scale MoE models.</li> </ul>"},{"location":"deepep/#3-deepep-architecture-key-components","title":"3. DeepEP Architecture &amp; Key Components","text":"Component Function Technical Details Dynamic Token Scheduler Manages token routing and communication between GPUs. - Batches tokens to minimize overhead.- Uses metadata (e.g., expert assignments) to optimize transfers. Optimized Collective Ops Accelerates GPU-to-GPU data transfers (e.g., all-to-all, reduce-scatter). - Custom CUDA/NCCL kernels for sparse MoE patterns.- Hierarchical communication (intra-node \u2192 inter-node). Load Balancer Ensures even distribution of tokens across experts. - Monitors expert workloads in real-time.- Uses auxiliary loss or heuristic reshuffling. Gradient Manager Handles gradient synchronization across experts. - Overlaps gradient computation and communication (asynchronous updates).- Sparse gradient aggregation. Fault Tolerance Recovers from node/GPU failures in distributed clusters. - Checkpointing and expert replication strategies."},{"location":"deepep/#4-how-deepep-works-step-by-step","title":"4. How DeepEP Works (Step-by-Step)","text":"<ol> <li>Input Routing:</li> <li>Tokens are processed by the gating network to determine expert assignments.</li> <li> <p>DeepEP\u2019s scheduler batches tokens by target GPU/expert.</p> </li> <li> <p>Communication Phase:</p> </li> <li>Optimized all-to-all transfers send tokens to the correct GPUs.</li> <li> <p>Overlaps communication with computation (e.g., expert processing).</p> </li> <li> <p>Expert Processing:</p> </li> <li> <p>Each GPU processes its assigned tokens locally using its subset of experts.</p> </li> <li> <p>Result Aggregation:</p> </li> <li>Results are routed back to the original GPUs using inverse all-to-all.</li> <li> <p>Gradients are synchronized with sparse aggregation to reduce bandwidth.</p> </li> <li> <p>Load Balancing:</p> </li> <li>Adjusts token distribution dynamically based on expert utilization metrics.</li> </ol>"},{"location":"deepep/#5-implementation-highlights","title":"5. Implementation Highlights","text":"<ul> <li>Integration: Compatible with PyTorch/TensorFlow via custom MoE layers.</li> <li>Kernel Optimizations:</li> <li>Low-level CUDA kernels for batched token transfers.</li> <li>Compressed data formats (e.g., FP16, sparse tensors) to reduce bandwidth.</li> <li>Adaptive Policies:</li> <li>Adjusts batch sizes and communication frequency based on cluster topology.</li> </ul>"},{"location":"deepep/#6-benefits-of-deepep","title":"6. Benefits of DeepEP","text":"<ul> <li>Reduced Latency: Up to 50% lower communication time vs. vanilla MPI/NCCL.</li> <li>Scalability: Supports 1000s of experts across 100s of GPUs.</li> <li>Higher Throughput: Achieves near-linear scaling in training speed.</li> <li>Ease of Use: Drop-in replacement for existing MoE implementations.</li> </ul>"},{"location":"deepep/#7-comparison-with-alternatives","title":"7. Comparison with Alternatives","text":"Library Optimized for MoE? Load Balancing Sparse Communication Scalability DeepEP Yes Dynamic heuristic Yes (custom kernels) 1000s of experts GShard Yes Auxiliary loss Partial 100s of experts FairScale Partial Basic No Moderate Vanilla NCCL No No No Limited"},{"location":"deepep/#8-use-cases-applications","title":"8. Use Cases &amp; Applications","text":"<ul> <li>Training large language models (LLMs) with MoE layers (e.g., Switch Transformers).</li> <li>Recommendation systems with dynamic input routing.</li> <li>Research on scalable and efficient distributed ML architectures.</li> </ul>"},{"location":"deepep/#9-demo-idea-for-class-project","title":"9. Demo Idea for Class Project","text":"<ul> <li>Visualization: Compare communication overhead with/without DeepEP in a toy MoE model.</li> <li>Code Snippet: Show how to replace standard PyTorch all-to-all with DeepEP\u2019s API.</li> <li>Benchmark: Profile throughput improvement on a multi-GPU setup (simulated or real).</li> </ul>"},{"location":"deepep/#10-discussion-points","title":"10. Discussion Points","text":"<ul> <li>How does DeepEP\u2019s token scheduling differ from traditional MPI all-to-all?</li> <li>What trade-offs exist between dynamic load balancing and computational overhead?</li> <li>Could DeepEP\u2019s techniques apply to non-MoE distributed training?</li> </ul> <p>Conclusion: DeepEP addresses critical bottlenecks in MoE training by co-designing communication strategies with MoE\u2019s unique workload patterns. Its optimizations enable scalable, efficient, and user-friendly distributed training for large MoE models.</p>"},{"location":"differences/","title":"Differences","text":"<p>Here\u2019s a structured breakdown of why DeepEP is tailored for MoE workloads and how it differs from general-purpose frameworks like FSDP (Fully Sharded Data Parallel) and DeepSpeed, which are optimized for dense models:</p>"},{"location":"differences/#1-core-differences-in-workload-patterns","title":"1. Core Differences in Workload Patterns","text":"<p>MoE Workloads are sparse and dynamic:</p> <ul> <li>Tokens are routed to different experts based on input (sparsity).</li> <li>Communication is dominated by all-to-all patterns (token redistribution).</li> <li>Load balancing is critical (experts may receive uneven token counts).</li> </ul> <p>Dense Model Workloads (e.g., standard Transformers):</p> <ul> <li>All GPUs process the same data/parameters (dense computation).</li> <li>Communication focuses on all-reduce (gradient synchronization).</li> <li>FSDP/DeepSpeed optimize memory and bandwidth for dense workloads.</li> </ul>"},{"location":"differences/#2-limitations-of-fsdpdeepspeed-for-moe","title":"2. Limitations of FSDP/DeepSpeed for MoE","text":"Framework MoE-Specific Challenges FSDP - Designed for parameter sharding, not dynamic token routing.- Overhead from frequent all-gather/reduce-scatter operations.- No native support for sparse all-to-all communication. DeepSpeed - MoE support exists but focuses on memory optimization (e.g., ZeRO-Offload).- Limited optimizations for token routing and load imbalance.- Relies on general-purpose NCCL collectives (not MoE-aware)."},{"location":"differences/#3-how-deepep-addresses-moe-specific-bottlenecks","title":"3. How DeepEP Addresses MoE-Specific Bottlenecks","text":""},{"location":"differences/#a-communication-efficiency","title":"A. Communication Efficiency","text":"<ul> <li>Optimized All-to-All:</li> <li>DeepEP uses sparse collective operations tailored for MoE\u2019s token routing.</li> <li>Reduces metadata overhead by batching tokens by target expert/GPU.</li> <li> <p>Example: Avoids padding tokens (common in frameworks like FairScale).</p> </li> <li> <p>Hierarchical Transfers:</p> </li> <li>Prioritizes intra-node communication before inter-node to reduce latency.</li> <li>Overlaps computation (expert processing) with communication.</li> </ul>"},{"location":"differences/#b-load-balancing","title":"B. Load Balancing","text":"<ul> <li>Dynamic Token Rescheduling:</li> <li>Monitors GPU/expert workloads in real-time.</li> <li>Reassigns tokens to underutilized experts during runtime (heuristic-based).</li> <li>Contrast: FSDP/DeepSpeed use static parallelism with no runtime load balancing.</li> </ul>"},{"location":"differences/#c-sparse-gradient-handling","title":"C. Sparse Gradient Handling","text":"<ul> <li>Synchronization Efficiency:</li> <li>Aggregates only relevant gradients (experts that processed tokens).</li> <li>Avoids dense synchronization (FSDP/DeepSpeed synchronize all parameters).</li> </ul>"},{"location":"differences/#d-scalability","title":"D. Scalability","text":"<ul> <li>Supports expert parallelism (experts sharded across GPUs) + data parallelism.</li> <li>Scales to 1000s of experts (FSDP/DeepSpeed struggle beyond 100s due to coordination overhead).</li> </ul>"},{"location":"differences/#4-when-to-use-fsdpdeepspeed-vs-deepep","title":"4. When to Use FSDP/DeepSpeed vs. DeepEP","text":"Use Case Recommended Framework Reason Training dense models FSDP/DeepSpeed Optimized for memory and bandwidth in dense workloads. Small-scale MoE (\u2264 8 GPUs) DeepSpeed (with MoE) Adequate for basic MoE but lacks DeepEP\u2019s load balancing/communication. Large-scale MoE (100s+ GPUs) DeepEP Handles dynamic routing, sparse communication, and load imbalance natively. Research on MoE optimizations DeepEP Built for co-designing communication and MoE architecture."},{"location":"differences/#5-key-trade-offs","title":"5. Key Trade-offs","text":"<ul> <li>FSDP/DeepSpeed:</li> <li>\u2705 Better for memory-constrained setups (ZeRO optimizations).</li> <li> <p>\u274c Suboptimal for MoE due to token routing overhead and lack of sparse collectives.</p> </li> <li> <p>DeepEP:</p> </li> <li>\u2705 2-5x faster communication for MoE workloads (varies with expert count).</li> <li>\u274c Specialized for MoE; less useful for dense models.</li> </ul>"},{"location":"differences/#6-example-scenario","title":"6. Example Scenario","text":"<p>Imagine training a Switch Transformer (MoE-based LLM):</p> <ul> <li>With DeepSpeed: Token routing uses generic all-to-all, leading to padding and load imbalance.</li> <li>With DeepEP: Tokens are batched by target GPU, sparse gradients are aggregated, and tokens are dynamically rebalanced.</li> </ul>"},{"location":"differences/#7-summary","title":"7. Summary","text":"<ul> <li>FSDP/DeepSpeed excel at memory optimization and dense model training but lack MoE-specific communication and load-balancing optimizations.</li> <li>DeepEP is co-designed with MoE to address:</li> <li>Sparse, dynamic token routing.</li> <li>Scalable all-to-all communication.</li> <li>Runtime load balancing.</li> </ul> <p>For your class project, highlight how workload-specific optimizations (DeepEP for MoE vs. FSDP/DeepSpeed for dense models) are critical in distributed training. Use the comparison table above to structure your discussion!</p>"},{"location":"inference/","title":"Inference","text":"<p>DeepEP\u2019s design is primarily optimized for training, but certain components can also benefit inference depending on the deployment setup. Here\u2019s a breakdown of how DeepEP\u2019s mechanisms apply to both phases:</p>"},{"location":"inference/#1-training-vs-inference-in-moe","title":"1. Training vs. Inference in MoE","text":"Phase Key Activities Communication Needs Training - Forward pass (token routing + expert computation).- Backward pass (gradient synchronization). Heavy all-to-all for token routing, gradient aggregation, and dynamic load balancing. Inference - Forward pass only (token routing + expert computation). Token routing and result aggregation (no gradients), but latency-critical."},{"location":"inference/#2-how-deepep-is-used-in-training","title":"2. How DeepEP is Used in Training","text":"<ul> <li>All Components Active:</li> <li>Dynamic Token Scheduler: Batches tokens for efficient cross-GPU routing.</li> <li>Load Balancer: Adjusts token distribution to avoid hotspots during backward/forward passes.</li> <li>Gradient Manager: Aggregates sparse gradients across distributed experts.</li> <li> <p>Optimized Collectives: Accelerates all-to-all for token/gradient transfers.</p> </li> <li> <p>Critical for Scalability:   Training involves iterative weight updates, requiring low-latency communication and gradient synchronization across thousands of experts. DeepEP\u2019s optimizations directly target these bottlenecks.</p> </li> </ul>"},{"location":"inference/#3-how-deepep-could-be-used-in-inference","title":"3. How DeepEP Could Be Used in Inference","text":"<p>While inference doesn\u2019t require gradient synchronization, token routing and expert computation still depend on efficient communication. DeepEP\u2019s inference usage depends on the deployment scenario:</p>"},{"location":"inference/#a-large-batch-inference-eg-offline-batch-processing","title":"A. Large-Batch Inference (e.g., offline batch processing)","text":"<ul> <li>Relevant Components:</li> <li>Dynamic Token Scheduler: Batches tokens to minimize communication rounds.</li> <li>Optimized All-to-All: Accelerates token redistribution (critical for latency).</li> <li>Load Balancer: Ensures even expert utilization to maximize throughput.</li> </ul>"},{"location":"inference/#b-low-latency-inference-eg-real-time-applications","title":"B. Low-Latency Inference (e.g., real-time applications)","text":"<ul> <li>Challenges:</li> <li>Small batch sizes reduce the effectiveness of batching.</li> <li>Load balancing is less critical (fewer tokens to route).</li> <li>Potential Use:</li> <li>Lightweight token routing with hierarchical communication (intra-node first).</li> <li>Sparse data formats to reduce transfer size.</li> </ul>"},{"location":"inference/#c-edgedecentralized-inference","title":"C. Edge/Decentralized Inference","text":"<ul> <li>DeepEP\u2019s fault tolerance and hierarchical communication could help in distributed edge deployments, but this is less common.</li> </ul>"},{"location":"inference/#4-why-inference-is-less-demanding","title":"4. Why Inference is Less Demanding","text":"<ul> <li>No Gradients: Eliminates gradient synchronization overhead (~50% of training communication).</li> <li>Static Workloads: Inference often uses fixed expert assignments (no dynamic rebalancing).</li> <li>Lower Batch Sizes: Reduces the need for aggressive batching (unless processing large offline batches).</li> </ul>"},{"location":"inference/#5-key-differences-in-deepep-usage","title":"5. Key Differences in DeepEP Usage","text":"Feature Training Inference Token Routing Dynamic, load-balanced, batched. Static or lightly optimized. Communication Patterns All-to-all (tokens + gradients). All-to-all (tokens only). Load Balancing Critical (avoids GPU stalls). Optional (depends on token distribution). Gradient Management Required (sparse aggregation). Not applicable. Fault Tolerance Critical (long-running jobs). Less critical (shorter sessions)."},{"location":"inference/#6-when-would-deepep-be-used-for-inference","title":"6. When Would DeepEP Be Used for Inference?","text":"<ul> <li>Large-Scale MoE Models: Deploying models like Switch Transformer or DeepSeek-MoE across multiple GPUs/nodes.</li> <li>High-Throughput Batch Inference: Processing thousands of requests in parallel (e.g., recommendation systems).</li> <li>Research on MoE Inference: Studying communication-efficient MoE serving.</li> </ul>"},{"location":"inference/#7-alternatives-for-inference","title":"7. Alternatives for Inference","text":"<ul> <li>NVIDIA Triton Inference Server: Generic serving with MoE support, but lacks DeepEP\u2019s MoE-specific optimizations.</li> <li>Custom Kernels: Hand-optimized CUDA for token routing (e.g., FasterTransformer).</li> <li>Generic Collectives: NCCL/MPI all-to-all, but slower than DeepEP\u2019s sparse-aware kernels.</li> </ul>"},{"location":"inference/#8-practical-takeaways-for-your-project","title":"8. Practical Takeaways for Your Project","text":"<ol> <li>Focus on Training: DeepEP\u2019s core value is in distributed training optimization.</li> <li>Inference Demo: If exploring inference, highlight how token routing (e.g., all-to-all) benefits from DeepEP\u2019s batched transfers.</li> <li>Compare Frameworks: Show how DeepEP reduces latency vs. NCCL in both training and inference.</li> </ol> <p>Conclusion: DeepEP is primarily a training-first library, but its communication optimizations (e.g., sparse all-to-all, token batching) can still improve inference performance for large-scale MoE deployments. For your class project, emphasize its training benefits, but mention inference use cases as an extension!</p>"},{"location":"midterm-prep/","title":"stuff to prepare (bill of materials)","text":""},{"location":"midterm-prep/#risc-vs-cisc","title":"RISC vs CISC","text":"<p>RISC and x86 are different architectures for computer processors that differ in the complexity of their instruction sets. RISC (Reduced Instruction Set Computing) is more efficient and uses less power, while x86 (Complex Instruction Set Computing) is more widely used. [1, 2, 3, 4, 5]</p> RISC x86 Instructions Simple instructions that execute in one clock cycle Complex instructions that can perform multiple tasks Design Simpler design that's easier to pipeline More complex design that's harder to design Power consumption More efficient and uses less power Less efficient and uses more power Performance High performance per watt Widely used for personal computers <p>RISC processors are good for battery-operated devices that need to be energy efficient. x86 processors are the most widely used instruction set for personal computers. [3, 4] RISC-V is a new architecture that's more user-friendly for designers. Western Digital uses RISC-V cores in its storage controllers to increase throughput while reducing power consumption. [5, 6]</p>"},{"location":"midterm-prep/#rpc-vs-grpc","title":"RPC vs gRPC","text":"<p>Remote Procedure Call (RPC) is a general concept, while gRPC is a specific implementation of RPC. gRPC is an open-source framework that's designed to be more efficient, flexible, and language-agnostic than traditional RPC. [1, 2, 3, 4, 5, 6, 7]</p> RPC gRPC Description A method for software applications to communicate over a network An implementation of RPC that uses HTTP/2 and Protocol Buffers Features Batching, broadcasting, callbacks, and select subroutine Authentication, bidirectional streaming, flow control, and more Benefits Allows applications to call remote functions as if they were local Faster, easier to implement, and more web-friendly than other RPC implementations <p>gRPC was developed by Google in 2015. It's an open-source framework that's used in distributed systems. [4] When choosing between RPC and gRPC, you can consider your application's requirements and whether gRPC's advantages outweigh the disadvantages of RPC. [8]</p>"},{"location":"midterm-prep/#microkernel-vs-monolithic-marcokernel","title":"microkernel vs monolithic (marcokernel)","text":"<p>Microkernels and macro (or monolithic) kernels are two different approaches to operating system (OS) kernel design. Here\u2019s how they compare:</p> Feature Microkernel Macro (Monolithic) Kernel Architecture Minimal, modular Large, integrated Performance Slower due to IPC overhead Faster due to direct function calls Stability More stable (isolated services) Less stable (crash affects entire system) Security Higher (less trusted code in kernel) Lower (more code in kernel) Maintainability Easier (modular structure) Harder (tightly coupled code) Example OS MINIX, QNX, Hurd Linux, Windows, BSD <p>In modern operating systems, there's often a hybrid approach, where kernels like Windows NT and macOS use a mix of microkernel and monolithic design principles to balance performance and modularity.</p>"},{"location":"midterm-prep/#actual-questions-by-swany","title":"actual questions by <code>swany</code>","text":"<p>Part 1</p> <p>Part 2</p> <p>Part 3</p> <p>Part 4</p>"},{"location":"midterm-prep/#references","title":"References","text":"<ol> <li>https://emteria.com/blog/arm-vs-x86</li> <li>https://premioinc.com/blogs/blog/risc-vs-cisc-harnessing-arm-and-x86-computing-solutions-for-rugged-edge-ai</li> <li>https://www.arm.com/glossary/risc</li> <li>https://www.linkedin.com/advice/3/what-benefits-drawbacks-x86-x64-architectures</li> <li>https://www.dfrobot.com/blog-13483.html</li> <li>https://medium.com/@techAstheticrisc-v-arm-and-x86-the-battle-for-dominance-in-the-future-of-computing-a579a7770b3c</li> <li>https://medium.com/@vinciabhinav7/whats-grpc-when-to-use-grpc-part-1-an-overview-86efd5fa78a6</li> <li>https://aws.amazon.com/compare/the-difference-between-grpc-and-rest/</li> <li>https://pandaquests.medium.com/differences-between-grpc-and-rpc-76d122104b4c</li> <li>https://blog.postman.com/grpc-vs-rest/</li> <li>https://konghq.com/blog/learning-center/what-is-grpc</li> <li>https://sites.ualberta.ca/dept/chemeng/AIX-43/share/man/info/C/a_doc_lib/aixprggd/progcomc/rpc_feat.htm</li> <li>https://en.wikipedia.org/wiki/GRPC</li> <li>https://www.linkedin.com/pulse/comparing-rpc-grpc-look-pros-cons-each-technology-hugo-pagniez</li> </ol>"},{"location":"part-1/","title":"Part 1 - Overview","text":""},{"location":"part-1/#historical-systems","title":"Historical Systems","text":""},{"location":"part-1/#distributed-operating-systems","title":"Distributed Operating Systems","text":"<ul> <li>Amoeba, Chorus, Mach</li> </ul> Feature Amoeba Chorus Mach Microkernel Minimal kernel, most code in user-space servers Smaller kernel, component-based Larger kernel with essential functionalities Distributed Support Optimized for distributed environments Supports distributed computing Supports heterogeneous clusters Customization Limited customization, focused on distributed model Highly customizable based on components Customizable through IPC mechanisms"},{"location":"part-1/#distributed-middlewareframeworks","title":"Distributed Middleware/Frameworks","text":"<p>CORBA</p> <ul> <li>Distributed Computing Environment</li> <li>Grid Computing</li> </ul>"},{"location":"part-1/#clusters","title":"Clusters","text":"<ul> <li>Single System Image</li> <li> <p>High availability clusters</p> </li> <li> <p>Execution Models</p> </li> </ul>"},{"location":"part-2/","title":"Part 2 - Processes","text":"<ul> <li>What does it take to run a program?</li> <li>What has to be stored about a process when it starts and stops?</li> </ul>"},{"location":"part-2/#threads","title":"Threads","text":"<ul> <li>What is a thread?</li> <li>How does a thread differ from a process?</li> </ul>"},{"location":"part-2/#functions-co-routines","title":"Functions / Co-routines","text":"<ul> <li>What does it take to run a function?</li> </ul>"},{"location":"part-2/#communicating-processes","title":"Communicating Processes","text":"<ul> <li>Messages allow for coordinating processes</li> <li>Shared memory is another approach</li> <li>Communicating Sequential Processes</li> </ul>"},{"location":"part-2/#virtualization-virtual-machines","title":"Virtualization (Virtual Machines)","text":"<ul> <li>What are Virtual Machines?</li> <li>Entities that act like a computer systems.</li> </ul>"},{"location":"part-2/#emulation","title":"Emulation","text":"<ul> <li>Software that mimics the operation of hardware by interpreting instructions.</li> <li>An interpreter manages translation and updates the state of the emulated   device based on the semantics of the instructions.</li> </ul>"},{"location":"part-2/#system-virtualization","title":"System Virtualization","text":"<ul> <li>Allows part of a machine to act as a distinct Virtual Machine, running native   code for efficiency.</li> </ul>"},{"location":"part-2/#paravirtualization","title":"Paravirtualization","text":"<ul> <li>Runs native code for user code and calls the local OS for privileged actions</li> </ul>"},{"location":"part-2/#servers-and-code-migration","title":"Servers and code migration","text":"<ul> <li>Another form of virtualization in a sense.</li> <li>Where do the code and data come from and where do they execute?</li> </ul>"},{"location":"part-2/#containers","title":"Containers","text":"<ul> <li>Another technology for providing Virtual Machines.</li> <li>Partitions a physical system differently</li> <li>Containers leverage existing systems to create VMs</li> <li>Elements are namespaces, control groups, and security policies.</li> </ul>"},{"location":"part-2/#orchestration","title":"Orchestration","text":"<ul> <li>We discussed in the context of containers, but orchestrators are not tied to   containers.</li> </ul>"},{"location":"part-2/#virtual-machines-aspects-of-use","title":"Virtual Machines - Aspects of Use","text":"<ul> <li>Adaptability, Elasticity</li> <li>Isolation of software dependencies</li> </ul>"},{"location":"part-3/","title":"Part 3 - Communication","text":"<ul> <li>Patterns, Paradigms, Programming, Protocols</li> </ul>"},{"location":"part-3/#sockets","title":"Sockets","text":"<ul> <li>System level communication interface</li> <li>Streams vs Datagrams</li> </ul>"},{"location":"part-3/#addressing","title":"Addressing","text":"<ul> <li>Resolving hostnames</li> </ul>"},{"location":"part-3/#mpi","title":"MPI","text":"<ul> <li>Message-oriented</li> <li>Maximizes potential efficiency</li> </ul>"},{"location":"part-3/#messaging-interfaces","title":"Messaging Interfaces","text":"<ul> <li>Variety of higher-level interfaces with different use targets</li> </ul>"},{"location":"part-3/#pubsub","title":"Pub/Sub","text":"<ul> <li>Publication and Subscription are decoupled via brokers</li> </ul>"},{"location":"part-3/#messaging-comparisons","title":"Messaging comparisons","text":"<ul> <li>Blocking vs Non-blocking</li> <li>Synchronous vs Asynchronous</li> <li>Direct vs Indirect</li> </ul> Characteristic Blocking Non-blocking Definition Sender is suspended until message is received or processed Sender continues execution after sending a message without waiting Control Flow Halts execution of sending process until operation completes Allows sending process to continue execution immediately Resource Utilization Can lead to inefficient use of CPU time while waiting Better CPU utilization as processes don't wait idle Programming Model Simpler programming model, sequential execution More complex programming with callbacks or polling Use Cases Critical operations where processing order matters I/O-bound applications, user interfaces, high-throughput systems Characteristic Synchronous Asynchronous Definition Communication follows a defined timing sequence Communication occurs without timing coordination between parties Message Handling Sender waits for receiver to process message Sender doesn't wait for receiver to process message Coordination Both parties must be available during communication Parties can operate independently at their own pace Performance Impact Can create performance bottlenecks Better throughput in high-latency environments Error Handling Immediate feedback on errors Error handling requires separate mechanisms Example Remote procedure calls, HTTP requests Message queues, email, event-driven architectures Characteristic Direct Indirect Addressing Messages sent to specific, known recipients Messages sent to intermediaries (queues, topics, channels) Coupling Tighter coupling between sender and receiver Looser coupling, participants may not know each other Scalability Limited by direct connections between components Better scalability as components don't need direct connections Fault Tolerance More vulnerable to receiver unavailability Better fault tolerance with message persistence Routing Control Limited routing options Flexible routing based on content, topics, or patterns Implementation Examples Socket communication, named pipes Message brokers, publish-subscribe systems, event buses"},{"location":"part-4/","title":"Part 4 - Networking","text":""},{"location":"part-4/#topologies","title":"Topologies","text":"<ul> <li>Data Center Network Topologies</li> <li>Multipath, Multistage, Multilink, challenges and benefits</li> </ul> Feature Multipath Multistage Multilink Challenges Benefits Definition Multiple paths for data transmission.  Data travels through several paths before reaching its destination. Series of optical switches or amplifiers to increase bandwidth and reduce signal loss. A single fiber optic link with multiple optical splitters to increase bandwidth and reduce distance. High cost, complex implementation, potential for signal degradation Increased bandwidth, reduced signal loss, improved reliability How it Works Data travels along multiple paths, often with different wavelengths. Multiple optical switches/amplifiers placed in a sequence. Optical splitters create multiple paths from a single fiber. Difficult to implement, requires careful alignment and control. Higher bandwidth, reduced signal loss, improved signal quality Complexity Relatively simple conceptually. Complex \u2013 requires careful design and control. Moderate \u2013 requires managing multiple optical paths. High complexity, requires specialized equipment and expertise. Simplified system design, lower implementation costs Typical Use High-bandwidth applications like video streaming, large data transfers. Longer distances, high-capacity networks. Dense-mode fiber optic networks, long-haul connections. High initial cost, difficult to troubleshoot, requires significant investment Reduced bandwidth costs, improved network capacity, increased distance capabilities Main Benefit Increased bandwidth &amp; reduced latency for certain applications. Increased bandwidth, improved reliability, reduced congestion. Increased bandwidth &amp; reduced cost for long-distance connections. High initial cost &amp; complexity, specialized expertise needed Enhanced network capacity, improved performance, reduced deployment costs"},{"location":"part-4/#virtualization","title":"Virtualization","text":"<ul> <li>Basic VLANs</li> <li>Other virtualization protocols and models</li> </ul>"},{"location":"part-4/#sdn-and-nfv","title":"SDN and NFV","text":"<ul> <li>Software Defined Networking</li> <li>Routers and Routing, Switches and Switching</li> <li>Match, Action pairs and Flow Tables</li> <li>What is a firewall? What is a NAT?</li> <li>Network Function Virtualization</li> </ul>"},{"location":"part-4/#software-defined-networking-sdn","title":"Software Defined Networking (SDN)","text":"Characteristic Traditional Networking Software Defined Networking Architecture Integrated control and data planes within each device Separation of control plane and data plane Control Distributed control across network devices Centralized control via SDN controller Programmability Limited programmability via CLI/SNMP Highly programmable via APIs and software interfaces Management Device-by-device configuration Network-wide policy implementation Flexibility Static configurations, limited automation Dynamic configuration, highly automated Innovation Pace Slow, dependent on vendor development cycles Rapid, software-driven innovation Protocols Vendor-specific or standard protocols (OSPF, BGP) OpenFlow, NETCONF, REST APIs Visibility Limited network-wide visibility Comprehensive network-wide visibility"},{"location":"part-4/#routers-and-routing-switches-and-switching","title":"Routers and Routing, Switches and Switching","text":"Characteristic Routers/Routing Switches/Switching OSI Layer Network layer (Layer 3) Data link layer (Layer 2) Addressing Uses IP addresses Uses MAC addresses Scope Between networks/subnets Within the same network/broadcast domain Decision Making Route selection based on IP addressing and routing tables Frame forwarding based on MAC address tables Protocols BGP, OSPF, RIP, EIGRP Spanning Tree Protocol (STP), VLANs Function Connects different networks and routes traffic between them Connects devices within a network segment Broadcast Handling Typically blocks broadcasts Forwards broadcasts within a VLAN Intelligence Higher processing overhead for packet inspection Lower processing overhead for frame switching Typical Use Internet connectivity, WAN connections LAN connectivity, workgroup connections"},{"location":"part-4/#match-action-pairs-and-flow-tables","title":"Match, Action Pairs and Flow Tables","text":"Characteristic Description Example Match Fields Packet header fields used for comparison Source/destination IP, MAC, ports, protocol type Actions Operations performed on matching packets Forward, drop, modify, count, queue Flow Table Structure Ordered collection of flow entries Priority, match fields, counters, actions, timeouts Processing Pipeline Sequential evaluation through multiple tables Packet enters \u2192 matched \u2192 actions applied \u2192 next table Entry Types Different rule types in flow tables Exact match, wildcard match, priority-based entries Controller Interaction When no match is found in flow tables Packet sent to controller, controller adds new flow entry Performance Factors Elements affecting flow table operation Table size, lookup speed, update frequency Security Considerations Security aspects of flow tables Flow rule verification, authentication of updates"},{"location":"part-4/#firewall-and-nat","title":"Firewall and NAT","text":"Characteristic Firewall Network Address Translation (NAT) Primary Purpose Security - filters traffic based on rules Addressing - maps private IPs to public IPs Function Controls incoming/outgoing network traffic Modifies packet headers to enable shared public IP usage Types Stateless, stateful, next-gen, WAF, proxy Static NAT, Dynamic NAT, PAT (Port Address Translation) OSI Layers Can operate at layers 3-7 depending on type Primarily operates at layers 3-4 Security Role Primary security device Security by obscuring internal addressing Configuration Focus Access control rules, inspection depth Address pools, port mapping, timeouts Typical Placement Network perimeter, segments, hosts Edge of private networks Scalability Impact Can become bottleneck under high traffic Can limit connection capacity due to port exhaustion Modern Implementations Integrated in UTM, cloud security groups Built into routers, load balancers, firewalls"},{"location":"part-4/#network-function-virtualization-nfv","title":"Network Function Virtualization (NFV)","text":"Characteristic Traditional Network Functions Network Function Virtualization Implementation Dedicated physical appliances Software running on standard hardware Hardware Dependency Purpose-built proprietary hardware Commodity x86 servers, storage, switches Deployment Time Days to weeks Minutes to hours Scalability Hardware addition/replacement required Dynamic scaling through virtualization Capital Expense High upfront costs for equipment Reduced CAPEX, pay-as-you-grow model Operational Expense Higher power, cooling, space requirements Lower operational costs through consolidation Function Examples Physical routers, firewalls, load balancers vRouter, vFirewall, vLoadBalancer, vIDS/IPS Management Often device-specific management interfaces Unified management platforms Service Chaining Manual, hardware-based connections Dynamic, software-defined service chaining Vendor Dependency Often locked to specific vendors Multi-vendor support on common infrastructure"},{"location":"project/","title":"Home","text":""},{"location":"project/#ideas","title":"ideas","text":"<ol> <li>DeepSeek - DeepSeek 3FS (the distributed file system)</li> <li>DeepEP</li> <li>Explaining Apple Chip and GPU cores (either M3 or M4 and GPU cores in a SOC vs dedicated GPUs)</li> </ol>"},{"location":"pytorch/","title":"Distributed training","text":"<p>I have spoken with Dr. Swany for my project</p> <p>Deven Mistry, demistry@iu.edu</p> <p>This article talks about distributed training in PyTorch</p>"},{"location":"pytorch/#ddp","title":"ddp","text":"<ol> <li>constructing the DDP module</li> <li>forward pass</li> <li>backward pass</li> <li>optimizer states</li> </ol>"},{"location":"pytorch/#things-to-talk-about","title":"things to talk about","text":"<ol> <li>processgroup c10d<ol> <li><code>ProcessGroup</code> manages the communication between different processes participating in distributed training. It handles operations like <code>all_reduce</code>, <code>broadcast</code>, <code>scatter</code>, etc.</li> </ol> </li> <li>backend type \"gloo\", \"nccl\", etc<ol> <li><code>nccl</code> \u2014 Optimized for NVIDIA GPUs, recommended for multi-GPU setups.</li> <li><code>gloo</code> \u2014 CPU and GPU compatible, often used for multi-node training when GPUs aren't available.</li> <li><code>mpi</code> \u2014 Uses the MPI library for communication (less commonly used nowadays).</li> </ol> </li> <li>data distribution using the <code>DistributedSampler</code><ol> <li>Ensures each process gets a unique, non-overlapping subset of the data.</li> <li>Typically used with a <code>DataLoader</code> to evenly split and shuffle the dataset.</li> </ol> </li> <li>Major piece: gradient synchronization<ol> <li>The core of DDP.</li> <li>During the backward pass, gradients are averaged across all GPUs using an all-reduce operation.</li> <li>Synchronization happens automatically when <code>.backward()</code> is called on the loss.</li> </ol> </li> <li>when to use this?<ol> <li>ideally when your model fits entirely on a single GPU</li> <li>used in cases when you have a lot of data, that you can split across multiple GPUs</li> </ol> </li> </ol>"},{"location":"pytorch/#advantages","title":"advantages","text":"<ol> <li>shards data across the <code>N</code> GPUs.</li> <li>not limited to using multiple GPUs on a single node, it will also work on multiple GPUs across multiple nodes</li> </ol>"},{"location":"pytorch/#disadvantages","title":"disadvantages","text":"<ol> <li>creates multiple copies of the model across the <code>N</code> gpus</li> <li>redundant use of memory because of multiple copies of the model, way to overcome this is with sharding</li> </ol>"},{"location":"pytorch/#links","title":"links","text":"<ol> <li>overview from the korean site.)</li> <li>ddp tutorial</li> <li>ddp notes</li> <li>medium article</li> </ol>"},{"location":"pytorch/#fsdp","title":"fsdp","text":"<p>Note: I might not cover FSDP because it introduces additional complexity \u2014 involving sharding policies, parameter scattering, optimizer state partitioning, and advanced collective operations like <code>ReduceScatter</code>. It\u2019s a powerful approach but deserves its own dedicated explanation.</p>"}]}